{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf640cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate environment is ready\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "import transformers\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "print(\"âœ… Scikit-learn OK\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"flytech/python-codes-25k\", split=\"train[:10]\")\n",
    "print(\"âœ… Datasets OK\")\n",
    "\n",
    "print (\" Environment Ready!!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ca90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and inital training Iteration 1\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:32\"\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Smaller model + dataset\n",
    "model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "dataset = load_dataset(\"flytech/python-codes-25k\", split=\"train[:200]\")  # Direct split\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model CRITICALLY with gradients enabled\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Enable training mode + gradients\n",
    "model.train()\n",
    "model.enable_input_require_grads()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Apply LoRA AFTER enabling gradients\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    # DeepSeek-V2 specific (MoE + MLA layers)\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Verify trainable params\n",
    "\n",
    "# tokenization (dynamic padding)\n",
    "def tokenize_function(examples):\n",
    "    texts = [f\"### Instruction: {inst}\\n### Input: {inp}\\n### Response: {out}\" \n",
    "             for inst, inp, out in zip(examples['instruction'], examples['input'], examples['output'])]\n",
    "    tokenized = tokenizer(texts, truncation=True, max_length=128)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# Ultra-safe TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_qwen\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    max_steps=50,  # Very small for testing\n",
    "    logging_steps=5,\n",
    "    dataloader_num_workers=0,\n",
    "    report_to=None,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=2e-4\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=None  # Let trainer handle\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Iteration 2\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:32\"\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Production model + smaller dataset for testing\n",
    "model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "dataset = load_dataset(\"flytech/python-codes-25k\", split=\"train[:1000]\")  # Reduced for testing\n",
    "\n",
    "# Load tokenizer FIRST\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 4-bit quantization + device_map + offload\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "cuda_device = torch.cuda.current_device()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # CRITICAL: 4-bit quantization\n",
    "    device_map={\"\": cuda_device},  # Force same GPU device for all layers\n",
    "    offload_folder=\"offload\",        # CRITICAL: CPU offload for excess layers\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing BEFORE LoRA\n",
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "#Freeze base model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Apply LoRA (only ~1% params trainable)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,                             # Reduced from 16\n",
    "    lora_alpha=16,                   # Reduced\n",
    "    lora_dropout=0.05,               # Reduced\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.base_model.enable_input_require_grads()\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenization (shorter max_length)\n",
    "def tokenize_function(examples):\n",
    "    texts = [f\"### Instruction: {inst}\\n### Input: {inp}\\n### Response: {out}\" \n",
    "             for inst, inp, out in zip(examples['instruction'], examples['input'], examples['output'])]\n",
    "    tokenized = tokenizer(\n",
    "        texts, \n",
    "        truncation=True, \n",
    "        padding=False,               # Let collator handle padding\n",
    "        max_length=256,              # Increased slightly but safe\n",
    "        return_tensors=None\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# Ultra-conservative TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-coder-finetuned\",\n",
    "    num_train_epochs=3,              # Reduced for testing\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,   # Reduced from 8\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=1e-4,              # Reduced\n",
    "    fp16=True,\n",
    "    logging_steps=50,                 # More frequent logging\n",
    "    max_steps=2000,\n",
    "    save_steps=550,\n",
    "    eval_strategy=\"no\",\n",
    "    warmup_steps=50,                 # Reduced\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,\n",
    "    dataloader_num_workers=0,\n",
    "    group_by_length=True,\n",
    "    max_grad_norm=0.3,               # Gradient clipping\n",
    "    dataloader_drop_last=True        # Drop incomplete batches\n",
    ")\n",
    "\n",
    "# Clear any remaining memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Use proper data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "print(\"ðŸš€ Starting training... Monitor: watch nvidia-smi -l 1\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(\"./my-python-code-llm-final_qwen\")\n",
    "print(\"âœ… Training completed! Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb2d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference test\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "from peft import PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "# Clear GPU cache and kill lingering processes\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Set memory allocator for fragmentation\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# 4-bit quantization (replaces torch_dtype=device_map)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # âœ… ~1.5GB vs 3GB\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# CPU-first load, then GPU move (avoids OOM during quantization)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=None,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "base_model = base_model.to(\"cuda\")\n",
    "\n",
    "\n",
    "# Prepare model for PEFT (gradient checkpointing)\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# Original PeftModel line\n",
    "model = PeftModel.from_pretrained(base_model, \"./my-python-code-llm-final_qwen\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-1.5B-Instruct\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_code(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "    return response\n",
    "\n",
    "# Test\n",
    "print(generate_code(\"Write a Python function to reverse a string leveraging the pandas library\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11777b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear Nvidia cache, garbage, Free up GTX VRAM\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.synchronize()\n",
    "print(f\"VRAM Free: {torch.cuda.memory_reserved()/1e9:.1f}GB\")  # Should show >4GB free\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Nvidia usage\n",
    "import os\n",
    "os.system(\"nvidia-smi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "print(generate_code(\"Write a Python function to calculate fibonacci sequence\"))\n",
    "print(f\"Generated in {time.time()-start:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86dbced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio front end for local network access\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "frontEnd = gr.Interface(\n",
    "    fn=generate_code,\n",
    "    inputs=gr.Textbox(label=\"Input\", lines=3),\n",
    "    outputs=gr.Textbox(label=\"Generated Code\", lines=10, scale=1),\n",
    "    title=\"Python SLM\"\n",
    ")\n",
    "frontEnd.launch(server_name=\"0.0.0.0\", share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea93697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Iteration 3 - Resume from previous LoRA model\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:32\"\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, \n",
    "    DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# Same base model and quantization\n",
    "model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "previous_model_path = \"./my-python-code-llm-final_qwen\"\n",
    "\n",
    "# Use flytech/python-codes-25k (works reliably, no script issues)\n",
    "print(\"Loading Python code dataset...\")\n",
    "dataset = load_dataset(\"flytech/python-codes-25k\", split=\"train[:5000]\")  # Back to your working dataset!\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#  Proper tokenization for python-codes-25k (uses 'text' field)\n",
    "def tokenize_function(examples):\n",
    "    texts = examples['text']  # flytech dataset uses 'text' field with Python code\n",
    "    tokenized = tokenizer(texts, truncation=True, padding=False, max_length=512, return_tensors=None)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# Same quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load BASE model (quantized)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, quantization_config=bnb_config,\n",
    "    device_map=\"auto\", offload_folder=\"offload\",\n",
    "    trust_remote_code=True, torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load your PREVIOUS LoRA adapters on top\n",
    "model = PeftModel.from_pretrained(base_model, previous_model_path)\n",
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Conservative resume training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-coder-finetuned-v2\",\n",
    "    num_train_epochs=2, per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8, optim=\"adamw_torch\",\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True, logging_steps=25, max_steps=3000,\n",
    "    save_steps=750, eval_strategy=\"no\",\n",
    "    warmup_steps=25, dataloader_pin_memory=False,\n",
    "    gradient_checkpointing=True, remove_unused_columns=False,\n",
    "    report_to=None, dataloader_num_workers=0,\n",
    "    group_by_length=True, max_grad_norm=0.3,\n",
    "    dataloader_drop_last=True, load_best_model_at_end=False\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, padding=True)\n",
    "\n",
    "print(\"ðŸš€ Resuming training from previous LoRA adapters...\")\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer, data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./my-python-code-llm-v2_final\")\n",
    "print(\"âœ… Iteration 3 complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wm_mlenv (PyTorch)",
   "language": "python",
   "name": "wm_mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
